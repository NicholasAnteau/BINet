# -*- coding: utf-8 -*-
"""BINET_Solution.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/138FyKCfMGj0lYZDxN0lzxzxzk1jiF8vj
"""

import torch
import numpy as np
import math
import random
from torch import nn
import matplotlib.pyplot as plt
def createMesh(ticks, start, end, device):
    base = torch.linspace(start, end, ticks)
    x = base.unsqueeze(1)
    result = torch.tensor([])
    for xi in base:
        yi = torch.ones((ticks, 1)) * xi
        yi = torch.cat((x, yi), 1)
        result = torch.cat((result, yi),0)
    return result.to(device)

def Print_Heatmap(y, n, model, rows):
  size = 20
  validate = createMesh(size,0,1, gpu)
  y1 = u(validate, y, n, model, 400)
  y2 = g_transformation(validate, x0)
  y20 = y2.cpu().detach().numpy()
  y10 = y1.cpu().detach().numpy()

  Z1 = np.zeros((size, size))
  j = 0
  for i, value in enumerate(y10):
     Z1[i % 20][j] = value
     if i % 20 == 19:
        j += 1

  Z2 = np.zeros((size, size))
  j = 0
  for i, value in enumerate(y20):
      Z2[i % 20][j] = value
      if i % 20 == 19:
          j += 1

  xAxis = np.linspace(0, 1, size)
  yAxis = np.linspace(0, 1, size)

  fs = 14 # font size
  plt.rc('font', size=fs)          # controls default text sizes
  plt.rc('axes', titlesize=fs)     # fontsize of the axes title
  plt.rc('axes', labelsize=fs)     # fontsize of the x and y labels
  plt.rc('xtick', labelsize=fs)    # fontsize of the tick labels
  plt.rc('ytick', labelsize=fs)    # fontsize of the tick labels
  plt.rc('legend', fontsize=fs)    # legend fontsize
  # ####### PLOT 1 #######
  fig = plt.figure(figsize=(16.0, 10.0))
  ax = plt.axes()
  # fig, axs = plt.subplots(2, 1, constrained_layout=True)

  plt.subplot(2,2,1)
  plt.gca().set_title('Trained Model Heat Map')
  plt.contourf(xAxis, yAxis, Z1, 20, cmap='RdGy')
  plt.colorbar()
  # plt.axes(1,2,1).set_title('title')

  plt.subplot(2,2,2)
  plt.gca().set_title('True Function Heat Map')
  plt.contourf(xAxis, yAxis, Z2, 20, cmap='RdGy')
  plt.colorbar()
  #axs[1].set_title('titel2')
  plt.savefig('PDE_ex' + str(epoch) + 'iterates.pdf', bbox_inches='tight')
  #The PDE we are trying to approximate 

def Chart_Loss_vs_Iteration(loss):
  iteration_count = len(loss)
  iterations = []
  current_count = 0
  for iteration in range(iteration_count):
      iterations.append(current_count)
      current_count+=1000
  fig, ax = plt.subplots()
  ax.set_ylabel("loss")
  ax.set_xlabel("iterations")
  ax.plot(iterations, loss)

  plt.show()
def g_transformation(x, x0):
    x = x - x0
    x = x ** 2
    x = torch.sum(x, dim=1)
    x = torch.log(x)
    return x
#Approximated solution to the PDE
def u(x, y, n, model, rows):
    u = torch.zeros((rows, 1), dtype=torch.float, device=gpu)
    for j in range(rows):
      convolution = torch.zeros((rows, 1), dtype=torch.float, device=gpu)
      for i in range(rows):
          a_x_y = a(x[j], y[i], n[i], 0)
          hy = torch.flatten(model(y[i]))
          convolution[i] = a_x_y*hy
      u[j] = torch.sum(convolution)
    u = torch.flatten(u)
    u = 4*u/rows
    return u

gpu = torch.device("cpu")
def g(x, y, dimension): 
    x = x - y
    x = torch.linalg.vector_norm(x, ord=1, dim = dimension)
    x = torch.log(x)/math.log(math.e)
    return x/(2*math.pi)
#Function that returns n dimensional tensor with values that exist in omega (within the boundary),
#which is [0, 1) for this program
def format_omega_values(values, columns, max, min):
    return (max-min)*torch.rand((values, columns), device = gpu, dtype = torch.float) + min
x0 = torch.tensor([-1,-2], dtype=torch.float, device=gpu)

#Unit outer normal vectors that are perpendicular to the boundary that y exists on 
def format_unit_outer_normal(values):
    x = torch.ones((values,1), dtype=torch.float, device = gpu)
    y = -1*torch.ones((values,1), dtype=torch.float, device = gpu)
    o = torch.zeros((values,1), dtype=torch.float, device = gpu)
    l1 = torch.cat((x, o), 1)
    l2 = torch.cat((y,o), 1)
    l3 = torch.cat((o,x), 1)
    l4 = torch.cat((o,y), 1)
    return torch.cat((l1,l2, l3, l4))
#Returns tensor that is filled with variables x and y that exist on the boundary
def format_boundary_values(values, rows, x_bound):
    ones = torch.ones((rows,1), dtype=torch.float, device = gpu)
    zeros = torch.zeros((rows,1), dtype=torch.float, device = gpu)
    #initalize y
    l1 = torch.cat((ones,values), 1)
    l2 = torch.cat((zeros,values), 1)
    l3 = torch.cat((values,ones), 1)
    l4 = torch.cat((values,zeros), 1)
    y = torch.cat((l1,l2,l3,l4)) 
    if (x_bound == True):
      #initalize random x
      x = torch.zeros((4*rows, 2), dtype=torch.float, device=gpu)
      for i in range(4*rows):
        x1 = random.randint(0,1)
        x2 = random.randint(0,1)
        boundary = [x1, x2]
        if (boundary == [0, 0]):
            x[i] = torch.tensor((0, random.uniform(0,1)), dtype=torch.float,device=gpu)
            if x[i, 0] == y[i, 0] and x[i, 1] == y[i, 1]:
              print("Warning x and y are equal")
        elif (boundary == [0, 1]):
            x[i] = torch.tensor((1, random.uniform(0,1)), dtype=torch.float,device=gpu)
            if x[i, 0] == y[i, 0] and x[i, 1] == y[i, 1]:
              print("Warning x and y are equal")
        elif (boundary == [1, 0]):
            x[i] = torch.tensor((random.uniform(0,1), 1), dtype=torch.float,device=gpu)
            if x[i, 0] == y[i, 0] and x[i, 1] == y[i, 1]:
              print("Warning x and y are equal")
        elif (boundary == [1, 1]):
            x[i] = torch.tensor((random.uniform(0,1), 0), dtype=torch.float,device=gpu)
            if x[i, 0] == y[i, 0] and x[i, 1] == y[i, 1]:
              print("Warning x and y are equal")
      return y, x
    else:
      return y
class BInet(nn.Module):
   def __init__ (self):
       super(BInet, self).__init__()
       self.model = nn.Sequential(
           nn.Linear(2, 15),
           nn.Tanh(),
           nn.Linear(15, 30),
           nn.ReLU(),
           nn.Linear(30, 30),
           nn.Tanh(),
           nn.Linear(30, 30),
           nn.ReLU(),
           nn.Linear(30, 30),
           nn.Tanh(),
           nn.Linear(30, 1),
       )   
   def forward(self, data):
       x_y = self.model(data)
       return x_y

#double layer potential 
def a(x, y, n, dim):
    dgdn = x-y
    dgdn = dgdn*n
    dgdn = torch.sum(dgdn, axis=dim)
    dgdn = dgdn/(torch.norm((x-y), 2, dim=dim)**2)
    return -1*dgdn/(math.pi*2)
'''def II(x, y, n, model, rows):
        dgdn = dg_dn(x, y, n, 1)
        hy = model(y)
        hy = torch.flatten(hy)-
        convolution = (dgdn*hy)
        return (2*convolution)/math.pi'''
def I(model, x, x0, degree):
    return torch.flatten(model(x)/2) - g_transformation(x, x0)

class BInetLoss(nn.Module):
   def __init__ (self):
      super(BInetLoss, self).__init__()
      self.MSE = nn.MSELoss()
   def forward(self, x, x0, y, z, n, model, rows) : 
      Ix = I(model, x, x0, 2)
      a_x_y = a(x, y, n, 1)*torch.flatten(model(y))
      a_x_z = a(x, z, n, 1)*torch.flatten(model(z))
      A = (4/rows)*torch.sum(Ix**2)
      B = (16/rows)*(torch.sum(Ix*a_x_y))
      C = (64/rows)*torch.sum(a_x_y*a_x_z)
      loss = A + 2*B + C
      return loss
def Train(epochs, x, x0, y, z, n, BInet_loss, model, y_test, n_test, loss):
    for iteration in range(epochs):
        Loss = BInet_loss(x, x0, y, z, n, model, 400000)
        optimizer.zero_grad()
        Loss.backward()
        optimizer.step()
        if (iteration % 1000 == 0):
            Loss = Loss.cpu().detach().numpy()
            Print_Heatmap(y_test, n_test, model, 400)
            loss.append(Loss)
            print(Loss)
    return model
model = BInet().to(torch.device(gpu))
model = model.to(torch.float)
y, x_boundary = format_boundary_values(format_omega_values(100000, 1, 1, 0), 100000, x_bound=True)
z = format_boundary_values(format_omega_values(100000, 1, 1, 0), 100000, x_bound=False)
n = format_unit_outer_normal(100000)
BInet_loss = BInetLoss()
optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)
epochs = 1
y_test = format_boundary_values(format_omega_values(100, 1, 1, 0), 100, x_bound=False)
n_test = format_unit_outer_normal(100)
loss = []
model.train()
model = Train(4000, x_boundary, x0, y, z, n, BInet_loss, model, y_test, n_test, loss)
Chart_Loss_vs_Iteration(loss)